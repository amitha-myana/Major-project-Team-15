{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["J0S2zEvF7q2-","hoBTu5-sqwww","gbuZKAcPxlzp"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setup Java Environment"],"metadata":{"id":"J0S2zEvF7q2-"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XXXWNN9PrjWn","outputId":"dff3c3c4-a094-472e-ff2a-7412f552b352","executionInfo":{"status":"ok","timestamp":1713506915104,"user_tz":-330,"elapsed":831,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Java Version:\n","openjdk version \"11.0.22\" 2024-01-16\n","OpenJDK Runtime Environment (build 11.0.22+7-post-Ubuntu-0ubuntu222.04.1)\n","OpenJDK 64-Bit Server VM (build 11.0.22+7-post-Ubuntu-0ubuntu222.04.1, mixed mode, sharing)\n"]}],"source":["#!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","print('Java Version:')\n","!java -version"]},{"cell_type":"code","source":["# Set JAVA_HOME environment variable for Java 11\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\""],"metadata":{"id":"OuPAafnQufl8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify Java installation and JAVA_HOME path\n","!echo $JAVA_HOME\n","!java -version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zzvW8S83uq2A","outputId":"a2fdafd3-24ad-49c9-9aa7-ac163d350ffe","executionInfo":{"status":"ok","timestamp":1713506915503,"user_tz":-330,"elapsed":4,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/lib/jvm/java-11-openjdk-amd64\n","openjdk version \"11.0.22\" 2024-01-16\n","OpenJDK Runtime Environment (build 11.0.22+7-post-Ubuntu-0ubuntu222.04.1)\n","OpenJDK 64-Bit Server VM (build 11.0.22+7-post-Ubuntu-0ubuntu222.04.1, mixed mode, sharing)\n"]}]},{"cell_type":"markdown","source":["# Install, Configure and Start SSH"],"metadata":{"id":"hoBTu5-sqwww"}},{"cell_type":"code","source":["!apt-get install -qq -o=Dpkg::Use-Pty=0 openssh-client openssh-server > /dev/null\n","!ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n","!cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n","!chmod 0600 ~/.ssh/authorized_keys\n","\n","!echo \"Host *\" > ~/.ssh/config\n","!echo \"  StrictHostKeyChecking no\" >> ~/.ssh/config\n","!echo \"  UserKnownHostsFile=/dev/null\" >> ~/.ssh/config\n","!chmod 400 ~/.ssh/config\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rsl6y3Obn8WX","outputId":"b8a3eae9-0456-4e84-f2da-47ef8820fc76","executionInfo":{"status":"ok","timestamp":1713506930773,"user_tz":-330,"elapsed":15272,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Creating config file /etc/ssh/sshd_config with new version\n","Creating SSH2 RSA key; this may take some time ...\n","3072 SHA256:mguLI8hJiXGtBUO242Qdv58pBQJWPV6tbaLVs5qSQKU root@50b1f6d22ce4 (RSA)\n","Creating SSH2 ECDSA key; this may take some time ...\n","256 SHA256:+hoXYa2PeUnloeEOUV6FxiISGiEF+LcAYTXGooc6IpE root@50b1f6d22ce4 (ECDSA)\n","Creating SSH2 ED25519 key; this may take some time ...\n","256 SHA256:EC+5QtuaH56LEeFZFWFR0PVhKWJSHe7u+DCSCDh0iVg root@50b1f6d22ce4 (ED25519)\n","invoke-rc.d: could not determine current runlevel\n","invoke-rc.d: policy-rc.d denied execution of start.\n","Created symlink /etc/systemd/system/sshd.service → /lib/systemd/system/ssh.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/ssh.service → /lib/systemd/system/ssh.service.\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","Generating public/private rsa key pair.\n","Created directory '/root/.ssh'.\n","Your identification has been saved in /root/.ssh/id_rsa\n","Your public key has been saved in /root/.ssh/id_rsa.pub\n","The key fingerprint is:\n","SHA256:ccQ+3yWEs3ES8MH9e/XE1R5cu06lZ5PSVp+GoJvUKwU root@50b1f6d22ce4\n","The key's randomart image is:\n","+---[RSA 3072]----+\n","|         .ooo+. +|\n","|         ...*.+o+|\n","|        .E...B ==|\n","|         o* o +.#|\n","|        So = +.&B|\n","|        . + o B++|\n","|         + .   ..|\n","|          .      |\n","|                 |\n","+----[SHA256]-----+\n"]}]},{"cell_type":"code","source":["!apt-get install -qq openssh-server\n","!service ssh start\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7rz-9yFFqCLh","outputId":"90942581-65b4-4588-a07f-025941a290b6","executionInfo":{"status":"ok","timestamp":1713506933249,"user_tz":-330,"elapsed":2481,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" * Starting OpenBSD Secure Shell server sshd\n","   ...done.\n"]}]},{"cell_type":"code","source":["!ssh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NZW0TLvgq078","outputId":"671e7661-b91a-4bf6-d5be-e75b0ef55cbd","executionInfo":{"status":"ok","timestamp":1713506933638,"user_tz":-330,"elapsed":392,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["usage: ssh [-46AaCfGgKkMNnqsTtVvXxYy] [-B bind_interface]\n","           [-b bind_address] [-c cipher_spec] [-D [bind_address:]port]\n","           [-E log_file] [-e escape_char] [-F configfile] [-I pkcs11]\n","           [-i identity_file] [-J [user@]host[:port]] [-L address]\n","           [-l login_name] [-m mac_spec] [-O ctl_cmd] [-o option] [-p port]\n","           [-Q query_option] [-R address] [-S ctl_path] [-W host:port]\n","           [-w local_tun[:remote_tun]] destination [command [argument ...]]\n"]}]},{"cell_type":"markdown","source":["# Download and Unpack Hadoop"],"metadata":{"id":"gbuZKAcPxlzp"}},{"cell_type":"code","source":["!wget -q https://archive.apache.org/dist/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz\n","!tar -xzf hadoop-3.2.1.tar.gz\n","!mv hadoop-3.2.1 /usr/local/hadoop\n"],"metadata":{"id":"PAnneKTgsHJo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop\"\n","os.environ[\"PATH\"] += \":/usr/local/hadoop/bin:/usr/local/hadoop/sbin\"\n"],"metadata":{"id":"xY2oSkqWx34y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Update hadoop-env.sh with JAVA_HOME for Java 11\n","!echo \"export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\" > /usr/local/hadoop/etc/hadoop/hadoop-env.sh"],"metadata":{"id":"kOrRoEiyu3vS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","os.environ[\"HDFS_NAMENODE_USER\"] = \"root\"\n","os.environ[\"HDFS_DATANODE_USER\"] = \"root\"\n","os.environ[\"HDFS_SECONDARYNAMENODE_USER\"] = \"root\"\n"],"metadata":{"id":"Vyhl4ggbt0ya"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!echo \"export HADOOP_SSH_OPTS=\\\"-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\\\"\" >> /usr/local/hadoop/etc/hadoop/hadoop-env.sh\n","# Set up core-site.xml\n","!echo \"<configuration>\\n\\t<property>\\n\\t\\t<name>fs.defaultFS</name>\\n\\t\\t<value>hdfs://localhost:9000</value>\\n\\t</property>\\n</configuration>\" > /usr/local/hadoop/etc/hadoop/core-site.xml\n","\n","# Set up hdfs-site.xml\n","!echo \"<configuration>\\n\\t<property>\\n\\t\\t<name>dfs.replication</name>\\n\\t\\t<value>1</value>\\n\\t</property>\\n\\t<property>\\n\\t\\t<name>dfs.namenode.name.dir</name>\\n\\t\\t<value>file:///usr/local/hadoop/data/hdfs/namenode</value>\\n\\t</property>\\n\\t<property>\\n\\t\\t<name>dfs.datanode.data.dir</name>\\n\\t\\t<value>file:///usr/local/hadoop/data/hdfs/datanode</value>\\n\\t</property>\\n</configuration>\" > /usr/local/hadoop/etc/hadoop/hdfs-site.xml\n"],"metadata":{"id":"BsUBxs0gqF9z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir -p /usr/local/hadoop/data/hdfs/namenode\n","!mkdir -p /usr/local/hadoop/data/hdfs/datanode\n"],"metadata":{"id":"LhQ-laNQulD3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!jps\n","\n","# Check what all nodes are running: Should list SecondaryNameNode, NameNode, DataNode, Jps"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0MPKQ2foUgXT","outputId":"e5995943-f0ac-4a2f-9896-ab2b5f07c990","executionInfo":{"status":"ok","timestamp":1713506970886,"user_tz":-330,"elapsed":407,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1287 Jps\n"]}]},{"cell_type":"code","source":["!/usr/local/hadoop/bin/hdfs namenode -format\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ao8Tf2G3nrog","outputId":"e511bc8a-c035-4faa-f006-78085d00354f","executionInfo":{"status":"ok","timestamp":1713506974647,"user_tz":-330,"elapsed":3766,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING: /usr/local/hadoop/logs does not exist. Creating.\n","2024-04-19 06:09:30,597 INFO namenode.NameNode: STARTUP_MSG: \n","/************************************************************\n","STARTUP_MSG: Starting NameNode\n","STARTUP_MSG:   host = 50b1f6d22ce4/172.28.0.12\n","STARTUP_MSG:   args = [-format]\n","STARTUP_MSG:   version = 3.2.1\n","STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-text-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.6.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-2.9.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-annotations-2.9.8.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-databind-2.9.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.13.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/local/hadoop/share/hadoop/common/lib/json-smart-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.4.10.jar:/usr/local/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.13.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/usr/local/hadoop/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.5.6.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang3-3.7.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.13.jar:/usr/local/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.11.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.18.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/local/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-5.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/local/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-3.2.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-kms-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-3.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-text-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-net-3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-2.9.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.9.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.9.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.10.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-security-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okio-1.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-io-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.2.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn:/usr/local/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/local/hadoop/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.9.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.9.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/objenesis-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/local/hadoop/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/usr/local/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-submarine-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.1.jar\n","STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z\n","STARTUP_MSG:   java = 11.0.22\n","************************************************************/\n","2024-04-19 06:09:30,681 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n","2024-04-19 06:09:30,892 INFO namenode.NameNode: createNameNode [-format]\n","Formatting using clusterid: CID-56e502c2-555f-4fea-adc3-323e47564f5a\n","2024-04-19 06:09:32,039 INFO namenode.FSEditLog: Edit logging is async:true\n","2024-04-19 06:09:32,064 INFO namenode.FSNamesystem: KeyProvider: null\n","2024-04-19 06:09:32,066 INFO namenode.FSNamesystem: fsLock is fair: true\n","2024-04-19 06:09:32,067 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n","2024-04-19 06:09:32,114 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\n","2024-04-19 06:09:32,114 INFO namenode.FSNamesystem: supergroup          = supergroup\n","2024-04-19 06:09:32,114 INFO namenode.FSNamesystem: isPermissionEnabled = true\n","2024-04-19 06:09:32,115 INFO namenode.FSNamesystem: HA Enabled: false\n","2024-04-19 06:09:32,189 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n","2024-04-19 06:09:32,206 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n","2024-04-19 06:09:32,206 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n","2024-04-19 06:09:32,213 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n","2024-04-19 06:09:32,214 INFO blockmanagement.BlockManager: The block deletion will start around 2024 Apr 19 06:09:32\n","2024-04-19 06:09:32,216 INFO util.GSet: Computing capacity for map BlocksMap\n","2024-04-19 06:09:32,216 INFO util.GSet: VM type       = 64-bit\n","2024-04-19 06:09:32,218 INFO util.GSet: 2.0% max memory 3.2 GB = 64.9 MB\n","2024-04-19 06:09:32,218 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n","2024-04-19 06:09:32,260 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n","2024-04-19 06:09:32,260 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n","2024-04-19 06:09:32,273 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\n","2024-04-19 06:09:32,273 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n","2024-04-19 06:09:32,273 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n","2024-04-19 06:09:32,273 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n","2024-04-19 06:09:32,274 INFO blockmanagement.BlockManager: defaultReplication         = 1\n","2024-04-19 06:09:32,274 INFO blockmanagement.BlockManager: maxReplication             = 512\n","2024-04-19 06:09:32,274 INFO blockmanagement.BlockManager: minReplication             = 1\n","2024-04-19 06:09:32,274 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n","2024-04-19 06:09:32,274 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n","2024-04-19 06:09:32,275 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n","2024-04-19 06:09:32,275 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n","2024-04-19 06:09:32,312 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n","2024-04-19 06:09:32,312 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n","2024-04-19 06:09:32,312 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n","2024-04-19 06:09:32,312 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n","2024-04-19 06:09:32,326 INFO util.GSet: Computing capacity for map INodeMap\n","2024-04-19 06:09:32,326 INFO util.GSet: VM type       = 64-bit\n","2024-04-19 06:09:32,327 INFO util.GSet: 1.0% max memory 3.2 GB = 32.5 MB\n","2024-04-19 06:09:32,327 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n","2024-04-19 06:09:32,343 INFO namenode.FSDirectory: ACLs enabled? false\n","2024-04-19 06:09:32,343 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n","2024-04-19 06:09:32,343 INFO namenode.FSDirectory: XAttrs enabled? true\n","2024-04-19 06:09:32,343 INFO namenode.NameNode: Caching file names occurring more than 10 times\n","2024-04-19 06:09:32,351 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n","2024-04-19 06:09:32,354 INFO snapshot.SnapshotManager: SkipList is disabled\n","2024-04-19 06:09:32,361 INFO util.GSet: Computing capacity for map cachedBlocks\n","2024-04-19 06:09:32,363 INFO util.GSet: VM type       = 64-bit\n","2024-04-19 06:09:32,364 INFO util.GSet: 0.25% max memory 3.2 GB = 8.1 MB\n","2024-04-19 06:09:32,364 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n","2024-04-19 06:09:32,384 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n","2024-04-19 06:09:32,385 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n","2024-04-19 06:09:32,385 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n","2024-04-19 06:09:32,391 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n","2024-04-19 06:09:32,391 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n","2024-04-19 06:09:32,395 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n","2024-04-19 06:09:32,395 INFO util.GSet: VM type       = 64-bit\n","2024-04-19 06:09:32,400 INFO util.GSet: 0.029999999329447746% max memory 3.2 GB = 997.2 KB\n","2024-04-19 06:09:32,400 INFO util.GSet: capacity      = 2^17 = 131072 entries\n","2024-04-19 06:09:32,464 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1003192383-172.28.0.12-1713506972449\n","2024-04-19 06:09:32,525 INFO common.Storage: Storage directory /usr/local/hadoop/data/hdfs/namenode has been successfully formatted.\n","2024-04-19 06:09:32,631 INFO namenode.FSImageFormatProtobuf: Saving image file /usr/local/hadoop/data/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\n","2024-04-19 06:09:32,885 INFO namenode.FSImageFormatProtobuf: Image file /usr/local/hadoop/data/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\n","2024-04-19 06:09:32,908 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n","2024-04-19 06:09:32,923 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n","2024-04-19 06:09:32,925 INFO namenode.NameNode: SHUTDOWN_MSG: \n","/************************************************************\n","SHUTDOWN_MSG: Shutting down NameNode at 50b1f6d22ce4/172.28.0.12\n","************************************************************/\n"]}]},{"cell_type":"code","source":["!/usr/local/hadoop/bin/hdfs --daemon start namenode\n","!/usr/local/hadoop/bin/hdfs --daemon start datanode\n","!/usr/local/hadoop/bin/hdfs --daemon start secondarynamenode\n"],"metadata":{"id":"CLeQa1p-qeOM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_ipython().system_raw('/usr/sbin/sshd -D &')\n"],"metadata":{"id":"gmG6hIvmoGCj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!/usr/local/hadoop/sbin/start-dfs.sh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"okEy929Gt--I","outputId":"12772186-844c-4f67-8cb4-ea090c111479","executionInfo":{"status":"ok","timestamp":1713507001635,"user_tz":-330,"elapsed":19851,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting namenodes on [localhost]\n","localhost: Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n","localhost: namenode is running as process 1399.  Stop it first.\n","Starting datanodes\n","localhost: Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n","localhost: datanode is running as process 1462.  Stop it first.\n","Starting secondary namenodes [50b1f6d22ce4]\n","50b1f6d22ce4: Warning: Permanently added '50b1f6d22ce4' (ED25519) to the list of known hosts.\n","50b1f6d22ce4: secondarynamenode is running as process 1525.  Stop it first.\n"]}]},{"cell_type":"code","source":["!/usr/local/hadoop/sbin/stop-dfs.sh\n","!/usr/local/hadoop/sbin/start-dfs.sh\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DV6gmVvburF4","outputId":"5fee766d-46c2-4b6f-a85a-f0293e118e2d","executionInfo":{"status":"ok","timestamp":1713507038278,"user_tz":-330,"elapsed":36681,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Stopping namenodes on [localhost]\n","localhost: Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n","Stopping datanodes\n","localhost: Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n","Stopping secondary namenodes [50b1f6d22ce4]\n","50b1f6d22ce4: Warning: Permanently added '50b1f6d22ce4' (ED25519) to the list of known hosts.\n","Starting namenodes on [localhost]\n","localhost: Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n","Starting datanodes\n","localhost: Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n","Starting secondary namenodes [50b1f6d22ce4]\n","50b1f6d22ce4: Warning: Permanently added '50b1f6d22ce4' (ED25519) to the list of known hosts.\n"]}]},{"cell_type":"code","source":["!jps\n","\n","# Check what all nodes are running: Should list SecondaryNameNode, NameNode, DataNode, Jps"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iXImQFQSoL4l","outputId":"7499069b-cc1b-4bb9-8cbd-ca506a2ed2a9","executionInfo":{"status":"ok","timestamp":1713507038279,"user_tz":-330,"elapsed":38,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3249 Jps\n","3010 SecondaryNameNode\n","2827 DataNode\n","2716 NameNode\n"]}]},{"cell_type":"code","source":["print('Hadoop Version:')\n","!/usr/local/hadoop/bin/hadoop version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gh2MRGrmvY4a","outputId":"4fa99451-46fa-4ada-eb0c-71406839abe1","executionInfo":{"status":"ok","timestamp":1713507038879,"user_tz":-330,"elapsed":630,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Hadoop Version:\n","Hadoop 3.2.1\n","Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842\n","Compiled by rohithsharmaks on 2019-09-10T15:56Z\n","Compiled with protoc 2.5.0\n","From source with checksum 776eaf9eee9c0ffc370bcbc1888737\n","This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-3.2.1.jar\n"]}]},{"cell_type":"markdown","source":["# Execute HDFS DFS Commands"],"metadata":{"id":"XGt6PE-Q9h2B"}},{"cell_type":"code","source":["!ls -ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lsoe645U-qTi","outputId":"9cc96327-2f26-42a9-fb1b-829a2a0d9f6b","executionInfo":{"status":"ok","timestamp":1713507039280,"user_tz":-330,"elapsed":406,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 350788\n","350784 -rw-r--r-- 1 root root 359196911 Jul  3  2020 hadoop-3.2.1.tar.gz\n","     4 drwxr-xr-x 1 root root      4096 Apr 17 13:29 sample_data\n"]}]},{"cell_type":"code","source":["!hdfs dfs -ls /"],"metadata":{"id":"KvKyqEXwuGDA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import logging\n","import os\n","\n","# Your existing setup\n","log_file_path = 'hadoop_log.log'  # Specify your log file path here\n","logger = logging.getLogger('HadoopRBACManager')\n","logger.setLevel(logging.INFO)  # Set log level to INFO\n","\n","file_handler = logging.FileHandler(log_file_path)\n","\n","log_format = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n","file_handler.setFormatter(log_format)\n","\n","logger.addHandler(file_handler)\n","\n","# Function to log the operation and message\n","def log_operation(operation, log_message):\n","    \"\"\"\n","    Logs the operation and message to the configured log file.\n","\n","    :param operation: The name of the operation performed.\n","    :param log_message: The message to be logged.\n","    \"\"\"\n","    logger.info(f\"Operation: {operation}, Message: {log_message}\")\n","\n","# Example usage\n","# log_operation(\"DatabaseUpdate\", \"Updating user data in database\")\n"],"metadata":{"id":"dI6tP-vc0fUA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# HDFS Helper Functions"],"metadata":{"id":"IqCHWAk5auL3"}},{"cell_type":"code","source":["import subprocess\n","import shlex\n","\n","def check_actual_file_owner(hdfs_path, supposed_owner):\n","    \"\"\"\n","    Check if the supposed owner is the actual owner of the file in HDFS.\n","\n","    :param hdfs_path: Path of the file in HDFS.\n","    :param supposed_owner: The username of the supposed owner.\n","    :return: Boolean indicating if the supposed owner is the actual owner.\n","    \"\"\"\n","    # Construct the HDFS list command\n","    ls_command = f\"hdfs dfs -ls {shlex.quote(hdfs_path)}\"\n","    try:\n","        # Execute the command and capture output\n","        process = subprocess.Popen(ls_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n","        output, error = process.communicate()\n","\n","        # Check for errors\n","        if process.returncode != 0:\n","            log_operation(\"OwnerCheckError\", f\"Error checking owner of {hdfs_path}: {error.decode()}\")\n","            return False\n","\n","        # Parse the owner from the command output\n","        output_lines = output.decode().strip().split('\\n')\n","        if output_lines:\n","            file_info = output_lines[-1].split()\n","            actual_owner = file_info[2]  # Owner is the third element in the listing\n","            return actual_owner == supposed_owner\n","        else:\n","            log_operation(\"OwnerCheckError\", f\"No information found for {hdfs_path}\")\n","            return False\n","    except Exception as e:\n","        log_operation(\"OwnerCheckException\", f\"Exception occurred while checking owner: {str(e)}\")\n","        return False"],"metadata":{"id":"UURkf-c9ABMQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import subprocess\n","import shlex\n","\n","def upload_to_hdfs(local_path, hdfs_path, hdfs_username):\n","    log_operation(f\"Upload\", f\"from: {local_path} to {hdfs_path} by user {hdfs_username}\")\n","\n","    # Set the HADOOP_USER_NAME environment variable\n","    env = os.environ.copy()\n","    env['HADOOP_USER_NAME'] = hdfs_username\n","\n","    # Properly escape local and HDFS paths\n","    escaped_local_path = shlex.quote(local_path)\n","    escaped_hdfs_path = shlex.quote(hdfs_path)\n","\n","    # Extract the parent directory of the target HDFS path\n","    parent_dir = os.path.dirname(hdfs_path)\n","    escaped_parent_dir = shlex.quote(parent_dir)\n","\n","    # Check if the parent directory exists\n","    check_parent_command = f\"hdfs dfs -test -e {escaped_parent_dir}\"\n","    parent_process = subprocess.Popen(check_parent_command, shell=True, env=env)\n","    parent_process.communicate()\n","\n","    # If the parent directory does not exist, create it\n","    if parent_process.returncode != 0:\n","        mkdir_parent_command = f\"hdfs dfs -mkdir -p {escaped_parent_dir}\"\n","        mkdir_parent_process = subprocess.Popen(mkdir_parent_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n","        mkdir_parent_out, mkdir_parent_err = mkdir_parent_process.communicate()\n","        if mkdir_parent_process.returncode != 0:\n","            print(f\"Error creating parent HDFS directory {parent_dir}: {mkdir_parent_err.decode()}\")\n","            return\n","\n","    # HDFS command to put (upload) the file\n","    put_command = f\"cat {escaped_local_path} | hdfs dfs -put - {escaped_hdfs_path}\"\n","    put_process = subprocess.Popen(put_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n","    put_out, put_err = put_process.communicate()\n","\n","    if put_process.returncode == 0:\n","        print(f\"Successfully uploaded {local_path} to {hdfs_path}\")\n","    else:\n","        print(f\"Error in uploading file to HDFS: {put_err.decode()}\")\n"],"metadata":{"id":"aR9frh0U9uXU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Permissions Dictionary: { 'file_path': { 'user': {'download': bool, 'delete': bool} } }\n","file_permissions = {}\n","\n","def grant_file_permission(hdfs_path, owner, user, download_perm=False, delete_perm=False):\n","    \"\"\"\n","    Grant permission to a user for specific operations on a file.\n","\n","    :param hdfs_path: Path of the file in HDFS.\n","    :param owner: The username of the owner of the file.\n","    :param user: Username to whom the permission is granted.\n","    :param download_perm: Boolean indicating if download permission is granted.\n","    :param delete_perm: Boolean indicating if delete permission is granted.\n","    \"\"\"\n","    if not check_actual_file_owner(hdfs_path, owner):\n","        log_operation(\"GrantPermissionError\", f\"User {owner} is not the actual owner of {hdfs_path}\")\n","        return\n","\n","    if hdfs_path not in file_permissions:\n","        file_permissions[hdfs_path] = {}\n","\n","    file_permissions[hdfs_path][user] = {\n","        'download': download_perm,\n","        'delete': delete_perm\n","    }\n","\n","    log_operation(\"GrantPermission\", f\"Permissions granted for {hdfs_path} to {user}: download={download_perm}, delete={delete_perm} by {owner}\")\n"],"metadata":{"id":"kPQ4M1dQAOgN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","\n","def upload_file_from_local_to_hdfs(hdfs_username):\n","    # Upload file from local PC to Colab\n","    uploaded_files = files.upload()\n","\n","    # Check if a file was uploaded\n","    if uploaded_files:\n","        # Assuming only one file is uploaded, get its file name\n","        local_filename = next(iter(uploaded_files))\n","\n","        # Call your function to upload the file from Colab to HDFS\n","        try:\n","            print(local_filename, f'/{local_filename}')\n","            upload_to_hdfs(local_filename, f'/{local_filename}', hdfs_username)\n","        except Exception as e:\n","            print(f\"An error occurred while uploading to HDFS: {e}\")\n","    else:\n","        print(\"No file was uploaded. Please try again.\")"],"metadata":{"id":"pbB9NYBja6YT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import subprocess\n","import shlex\n","\n","def delete_from_hdfs(hdfs_path, hdfs_username):\n","\n","    # Set the HADOOP_USER_NAME environment variable\n","    env = os.environ.copy()\n","    env['HADOOP_USER_NAME'] = hdfs_username\n","\n","    # Properly escape HDFS path\n","    escaped_hdfs_path = shlex.quote(hdfs_path)\n","\n","    # Check file ownership\n","    ls_command = f\"hdfs dfs -ls {escaped_hdfs_path}\"\n","    ls_process = subprocess.Popen(ls_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n","    ls_out, ls_err = ls_process.communicate()\n","\n","    if ls_process.returncode == 0:\n","        file_info = ls_out.decode().split()\n","        owner = file_info[2]\n","        log_operation(f\"Delete\", f\"file: {hdfs_path} owner: {owner} user: {hdfs_username}\")\n","        key_val = file_permissions.get(hdfs_path, {}).get(hdfs_username, {}).get('delete', False)\n","        if owner == hdfs_username or key_val:\n","            # User is the owner, proceed to delete\n","            env['HADOOP_USER_NAME'] = owner\n","            rm_command = f\"hdfs dfs -rm {escaped_hdfs_path}\"\n","            rm_process = subprocess.Popen(rm_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n","            rm_out, rm_err = rm_process.communicate()\n","\n","            if rm_process.returncode == 0:\n","                print(f\"Successfully deleted {hdfs_path}\")\n","                log_operation(f\"Delete\", f\"Successfully deleted {hdfs_path}\")\n","            else:\n","                print(f\"Error deleting file from HDFS: {rm_err.decode()}\")\n","                log_operation(f\"Delete\", f\"Error deleting file from HDFS: {rm_err.decode()}\")\n","        else:\n","            print(f\"User {hdfs_username} does not have permission to delete {hdfs_path}.\")\n","            log_operation(f\"Delete\", f\"User {hdfs_username} does not have permission to delete {hdfs_path}. \")\n","\n","    else:\n","        print(f\"Unable to retrieve file information for {hdfs_path}: {ls_err.decode()}\")\n","        log_operation(f\"Delete\", f\"Unable to retrieve file information for {hdfs_path}: {ls_err.decode()}\")"],"metadata":{"id":"li2zMffYu6oX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","import os\n","import subprocess\n","import shlex\n","import random\n","import string\n","import time\n","import json\n","import csv\n","\n","def generate_plausible_content(extension):\n","    if extension == 'pdf':\n","        # Simulating PDF content with text (since actual PDF generation is complex)\n","        return \"Report generated on [Date].\\nAnalysis Summary:\\n...\\n\"\n","    elif extension == 'json':\n","        sample_data = {\"id\": 123, \"status\": \"completed\", \"details\": {\"info\": \"user job\"}}\n","        return json.dumps(sample_data, indent=4)\n","    elif extension == 'csv':\n","        sample_data = \"id,name,value\\n1,SampleA,100\\n2,SampleB,200\\n\"\n","        return sample_data\n","    else:\n","        return \"Document created on [Date].\\nContent:\\n...\\n\"\n","\n","import os\n","import shlex\n","import subprocess\n","import tempfile\n","from datetime import datetime\n","def download_hdfs_file(hdfs_path, current_user):\n","    env = os.environ.copy()\n","    env['HADOOP_USER_NAME'] = current_user\n","\n","    # Check file ownership\n","    ls_command = f\"hdfs dfs -ls {shlex.quote(hdfs_path)}\"\n","    ls_process = subprocess.Popen(ls_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n","    ls_out, ls_err = ls_process.communicate()\n","\n","    if ls_process.returncode == 0:\n","        file_info = ls_out.decode().split()\n","        owner = file_info[2]\n","        log_operation(f\"Download\", f\"file: {hdfs_path} owner: {owner} user: {current_user}\")\n","        key_val = file_permissions.get(hdfs_path, {}).get(current_user, {}).get('download', False)\n","\n","        if owner == current_user or key_val:\n","            # Current user is the owner, provide original file for download\n","            env['HADOOP_USER_NAME'] = current_user\n","            local_filename = os.path.basename(hdfs_path)\n","            timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n","            temp_dir = tempfile.mkdtemp()\n","            temp_filepath = os.path.join(temp_dir, f\"{timestamp}_{local_filename}\")\n","            copy_command = f\"hdfs dfs -get {shlex.quote(hdfs_path)} {shlex.quote(temp_filepath)}\"\n","            subprocess.Popen(copy_command, shell=True, env=env).communicate()\n","            print(\"Downloading the file...\")\n","            files.download(temp_filepath)\n","\n","        else:\n","            # Current user is not the owner, generate and provide plausible file for download\n","            filename_with_extension = os.path.basename(hdfs_path)\n","            extension = os.path.splitext(filename_with_extension)[1][1:].lower()  # Extract file extension without dot\n","            plausible_content = generate_plausible_content(extension)\n","            timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n","            temp_dir = tempfile.mkdtemp()\n","            temp_filepath = os.path.join(temp_dir, f\"{timestamp}_{filename_with_extension}\")\n","            with open(temp_filepath, 'w') as temp_file:\n","                temp_file.write(plausible_content)\n","            print(\"Downloading the document...\")\n","            files.download(temp_filepath)\n","\n","\n","    else:\n","        print(f\"Unable to retrieve file information for {hdfs_path}: {ls_err.decode()}\")\n","        log_operation(f\"Download\", f\"Unable to retrieve file information for {hdfs_path}: {ls_err.decode()}\")\n"],"metadata":{"id":"Ad-ElAgN-ESE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Demo"],"metadata":{"id":"mBSVeFCfa0kH"}},{"cell_type":"code","source":["!hdfs dfs -ls /"],"metadata":{"id":"OiROzckhvSsB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713507317986,"user_tz":-330,"elapsed":3433,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}},"outputId":"a2165ac7-6721-44f0-e064-9adf5bedb200"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 3 items\n","-rw-r--r--   1 user03 supergroup       1697 2024-04-19 06:11 /anscombe.json\n","-rw-r--r--   1 user01 supergroup     301141 2024-04-19 06:10 /california_housing_test.csv\n","-rw-r--r--   1 user04 supergroup      33349 2024-04-19 06:12 /proj.png\n"]}]},{"cell_type":"code","source":["# Making root accessible to all users\n","!hdfs dfs -chmod 777 /"],"metadata":{"id":"6crhdX0_pY5F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["upload_to_hdfs('/content/sample_data/california_housing_test.csv', '/california_housing_test.csv', 'user01')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Auanbw9D9yqf","outputId":"b644df1b-f1f8-45d2-8ab4-41c9bb55bbdd","executionInfo":{"status":"ok","timestamp":1713507058232,"user_tz":-330,"elapsed":7355,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:HadoopRBACManager:Operation: Upload, Message: from: /content/sample_data/california_housing_test.csv to /california_housing_test.csv by user user01\n"]},{"output_type":"stream","name":"stdout","text":["Successfully uploaded /content/sample_data/california_housing_test.csv to /california_housing_test.csv\n"]}]},{"cell_type":"code","source":["upload_to_hdfs('/content/sample_data/mnist_test.csv', '/mnist_test.csv', 'user02')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aKucgSOjpqBy","outputId":"fdfe5e5d-6f05-4400-f8f8-3d46ca6f9ceb","executionInfo":{"status":"ok","timestamp":1713507065993,"user_tz":-330,"elapsed":7787,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:HadoopRBACManager:Operation: Upload, Message: from: /content/sample_data/mnist_test.csv to /mnist_test.csv by user user02\n"]},{"output_type":"stream","name":"stdout","text":["Successfully uploaded /content/sample_data/mnist_test.csv to /mnist_test.csv\n"]}]},{"cell_type":"code","source":["upload_to_hdfs('/content/sample_data/anscombe.json', '/anscombe.json', 'user03')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hHdNLEe6Yfxk","outputId":"a9f75e5e-2f5d-421f-fc71-9d5092e88f02","executionInfo":{"status":"ok","timestamp":1713507073774,"user_tz":-330,"elapsed":7791,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:HadoopRBACManager:Operation: Upload, Message: from: /content/sample_data/anscombe.json to /anscombe.json by user user03\n"]},{"output_type":"stream","name":"stdout","text":["Successfully uploaded /content/sample_data/anscombe.json to /anscombe.json\n"]}]},{"cell_type":"code","source":["# Example usage:\n","upload_file_from_local_to_hdfs(hdfs_username='user04')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":125},"id":"kgFc-UHHXi4l","outputId":"7a8c53ba-a5c3-4b8e-cb4c-efbe545663f3","executionInfo":{"status":"ok","timestamp":1713507127182,"user_tz":-330,"elapsed":53452,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-9b0b89c3-5e82-4784-be1f-0d2fd2090db5\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-9b0b89c3-5e82-4784-be1f-0d2fd2090db5\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:HadoopRBACManager:Operation: Upload, Message: from: proj.png to /proj.png by user user04\n"]},{"output_type":"stream","name":"stdout","text":["Saving proj.png to proj.png\n","proj.png /proj.png\n","Successfully uploaded proj.png to /proj.png\n"]}]},{"cell_type":"markdown","source":["Delete Files: Only owner can delete a file"],"metadata":{"id":"UGKaxtSgwm0-"}},{"cell_type":"code","source":["delete_from_hdfs('/mnist_test.csv', 'user01')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mcNhxSlgqgqW","outputId":"720a87da-a158-40d2-b931-fe05600fb753","executionInfo":{"status":"ok","timestamp":1713507129916,"user_tz":-330,"elapsed":2738,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:HadoopRBACManager:Operation: Delete, Message: file: /mnist_test.csv owner: user02 user: user01\n","INFO:HadoopRBACManager:Operation: Delete, Message: User user01 does not have permission to delete /mnist_test.csv. Owner is user02.\n"]},{"output_type":"stream","name":"stdout","text":["User user01 does not have permission to delete /mnist_test.csv. Owner is user02.\n"]}]},{"cell_type":"code","source":["grant_file_permission(hdfs_path='/mnist_test.csv', owner='user02', user='user01', download_perm=True, delete_perm=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zLjGJq-mBzHN","outputId":"6ea50a9e-ea08-4061-9555-74fd309c277d","executionInfo":{"status":"ok","timestamp":1713507133144,"user_tz":-330,"elapsed":3232,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:HadoopRBACManager:Operation: GrantPermission, Message: Permissions granted for /mnist_test.csv to user01: download=True, delete=True by user02\n"]}]},{"cell_type":"code","source":["delete_from_hdfs('/mnist_test.csv', 'user01')\n"],"metadata":{"id":"08PcXsTeCKSm","outputId":"e40082e6-3973-4717-c321-51f0f0142e1d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713507140897,"user_tz":-330,"elapsed":7760,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:HadoopRBACManager:Operation: Delete, Message: file: /mnist_test.csv owner: user02 user: user01\n","INFO:HadoopRBACManager:Operation: Delete, Message: Successfully deleted /mnist_test.csv\n"]},{"output_type":"stream","name":"stdout","text":["Successfully deleted /mnist_test.csv\n"]}]},{"cell_type":"code","source":["delete_from_hdfs('/mnist_test.csv', 'user02')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z3-LW8kJtgYa","outputId":"396eba35-3add-438c-85b4-e4032491d0f6","executionInfo":{"status":"ok","timestamp":1713507144016,"user_tz":-330,"elapsed":3148,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:HadoopRBACManager:Operation: Delete, Message: Unable to retrieve file information for /mnist_test.csv: ls: `/mnist_test.csv': No such file or directory\n","\n"]},{"output_type":"stream","name":"stdout","text":["Unable to retrieve file information for /mnist_test.csv: ls: `/mnist_test.csv': No such file or directory\n","\n"]}]},{"cell_type":"markdown","source":["Honeypot Access"],"metadata":{"id":"iLTh2-tEwtYa"}},{"cell_type":"code","source":["# Example usage\n","download_hdfs_file('/anscombe.json', 'user02')"],"metadata":{"id":"rzTQJGNZHefw","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"bd07e26a-3788-49b8-f069-8447a10f12fe","executionInfo":{"status":"ok","timestamp":1713507147088,"user_tz":-330,"elapsed":3103,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:HadoopRBACManager:Operation: Download, Message: file: /anscombe.json owner: user03 user: user02\n"]},{"output_type":"stream","name":"stdout","text":["Downloading the document...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_e07ec130-0b04-4975-9aa9-19b8d75fde9a\", \"20240419061225_anscombe.json\", 95)"]},"metadata":{}}]},{"cell_type":"code","source":["# Example usage\n","download_hdfs_file('/anscombe.json', 'user02')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"MVkB2nK7wvj7","outputId":"d5293127-e50a-4f80-ad06-6f4d00763d58","executionInfo":{"status":"ok","timestamp":1713507152087,"user_tz":-330,"elapsed":5009,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:HadoopRBACManager:Operation: Download, Message: file: /anscombe.json owner: user03 user: user02\n"]},{"output_type":"stream","name":"stdout","text":["Downloading the document...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_6415f8ba-2b33-4837-b0e3-9f5f6f8e8e3d\", \"20240419061230_anscombe.json\", 95)"]},"metadata":{}}]},{"cell_type":"code","source":["grant_file_permission(hdfs_path='/anscombe.json', owner='user03', user='user02', download_perm=True, delete_perm=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZHHHGaIDAV8q","outputId":"496d0832-c67a-42c0-8312-1c1a2c42ea18","executionInfo":{"status":"ok","timestamp":1713507155279,"user_tz":-330,"elapsed":3224,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:HadoopRBACManager:Operation: GrantPermission, Message: Permissions granted for /anscombe.json to user02: download=True, delete=False by user03\n"]}]},{"cell_type":"code","source":["# Example usage: after user03 granted access to user02\n","download_hdfs_file('/anscombe.json', 'user02')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"YSuq2z-PBhPU","outputId":"12bf317e-3278-4788-ace6-720e02d41a04","executionInfo":{"status":"ok","timestamp":1713507164045,"user_tz":-330,"elapsed":8771,"user":{"displayName":"Amitha Myana","userId":"16707113283012983837"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:HadoopRBACManager:Operation: Download, Message: file: /anscombe.json owner: user03 user: user02\n"]},{"output_type":"stream","name":"stdout","text":["Downloading the file...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_8af95d4e-24c5-4226-98ac-fc52b63c46f0\", \"20240419061237_anscombe.json\", 1697)"]},"metadata":{}}]}]}